# yspeech

CLI-утилита для распознавания длинных аудиозаписей (до 8--10 часов) через Yandex SpeechKit API v3 с диаризацией (разделением по спикерам).

## Требования

- GHC >= 9.0, cabal
- ffmpeg, ffprobe (в `$PATH`)
- Аккаунт Yandex Cloud:
  - сервисный аккаунт с ролью `ai.speechkit-stt.user`
  - IAM-токен или API-ключ

## Сборка

```bash
cabal build
```

## Использование

```
yspeech (-i|--input FILE) [-o|--output FILE] [--folder-id ID]
        [--iam-token TOKEN] [--api-key KEY] [--language LANG]
        [--model MODEL] [--chunk-hours HOURS] [--timestamps] [-v|--verbose]
```

### Опции

| Опция | Описание | По умолчанию |
|---|---|---|
| `-i`, `--input FILE` | Входной MP3-файл | (обязательно) |
| `-o`, `--output FILE` | Файл для записи результата | stdout |
| `--folder-id ID` | Folder ID в Yandex Cloud | env `YANDEX_FOLDER_ID` |
| `--iam-token TOKEN` | IAM-токен для аутентификации | env `YANDEX_IAM_TOKEN` |
| `--api-key KEY` | API-ключ (альтернатива IAM) | env `YANDEX_API_KEY` |
| `--language LANG` | Язык распознавания | `ru-RU` |
| `--model MODEL` | Модель распознавания | `general` |
| `--chunk-hours HOURS` | Макс. длительность чанка (часы) | `3.5` |
| `--timestamps` | Добавлять таймкоды `[HH:MM:SS]` | выключено |
| `-v`, `--verbose` | Подробный вывод | выключено |

Каждую опцию с пометкой `env ...` можно задать через переменную окружения вместо флага.

### Примеры

Базовое использование с API-ключом:

```bash
yspeech -i lecture.mp3 \
  --folder-id b1gabcdef123 \
  --api-key AQVNabcdef... \
  -o lecture.txt
```

С переменными окружения и таймкодами:

```bash
export YANDEX_FOLDER_ID=b1gabcdef123
export YANDEX_API_KEY=AQVNabcdef...

yspeech -i interview.mp3 -o result.txt --timestamps
```

### Формат вывода

```
Спикер 1: Здравствуйте, я хотел бы обсудить...
Спикер 2: Да, конечно, давайте начнём с...
Спикер 1: Первый вопрос касается...
```

С флагом `--timestamps`:

```
[00:00:15] Спикер 1: Здравствуйте, я хотел бы обсудить...
[00:00:22] Спикер 2: Да, конечно, давайте начнём с...
[00:01:03] Спикер 1: Первый вопрос касается...
```

## Как это работает

1. Входной MP3 разрезается на чанки по ~3.5 часа (лимит SpeechKit -- 4 часа)
2. Каждый чанк отправляется напрямую в SpeechKit API (base64 в теле запроса)
3. Для каждого чанка запускается асинхронное распознавание с включённой диаризацией
4. Утилита опрашивает API до завершения распознавания (~10 сек на 1 мин аудио)
5. Результаты склеиваются с коррекцией таймкодов и выводятся с метками спикеров

## Ограничения

- Диаризация SpeechKit поддерживает не более 2 спикеров
- Аудио должно быть моно для корректной работы диаризации
